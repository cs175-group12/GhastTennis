{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "16eb897c26cdfcf18817bc60a8e0737e3939ff1e8491198c807979170104e811"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random([1000]) #coming up with arbitrary data and a predictor \n",
    "data = np.reshape(data, (100,10)) #reshape to be 100 rows of 10 cols\n",
    "predict = np.zeros(100)\n",
    "for i in range(len(data)):\n",
    "    predict[i] = float(np.random.random()-.5)\n",
    "    data[i] /= np.sum(data[i]) + predict[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing : \n",
    "'''     network : dense                    matrices                                  rows correspond to input layer size + bias, columns to output layer size\n",
    "                                                                                     X is a bias, it should be a constant 1. pad it onto the back of output.\n",
    "        1       2       3                    1->2           2->3\n",
    "        [a]             [f]                 [1 , 2         [1, 2, 3 \n",
    "        [b]     [d]     [g]                  3 , 4          4, 5, 6]\n",
    "        [c]     [e]     [h]                  5 , 6]          \n",
    "        [X]     [X]     [X]                  \n",
    "        \n",
    "        forward propagation                                                         each row of input1 corresponds to a row of output. \n",
    "                           [1, 2                                                    each col of input2 corresponds to a col of output\n",
    "                            3, 4   =      [1a + 3b + 5c  , 2a + 4b + 6c]  = [_d,_e]                                           \n",
    "        [a, b, c]    x      5, 6                                                    [1,input layer] @ [input layer, outputlayer] = [1,ouputlayer]\n",
    "                            7, 8]                                                   intuitively, each neuron is represented by a column of the connecting matrix\n",
    "                                                                                    with each row within representing an axon from each row in input. \n",
    "          \n",
    "        intuitively, neurons[i] @ axons[i] = neurons[i+1]\n",
    "        [d,e] = activation_function([_d,_e] + bias )                                leaky rel u, softmax, and sigmoid were tested as activation functions. \n",
    "                                                                                    sigmoid produces best results. \n",
    "        \n",
    "\n",
    "        back propagation\n",
    "            Error : [e1, e2, e3] = [?-f , ?-g , ?-h]\n",
    "            neurons : [[input1, input2, input3] , [d, e] , [f, g, h]]\n",
    "            axons[i-1] += -1 * learning_rate * [error] @ transpose(neurons[i-1])\n",
    "            \n",
    "\n",
    "                                   [e1                           [e1 * f, e1 * g, e1 * h\n",
    "        transpose(error) @ value =  e2    x    [f, g, h]    =     e2 * f, e2 * g, e2 * h\n",
    "                                    e3]                           e3 * f, e3 * g, e3 * h]\n",
    "\n",
    "        calculate new d,e from errors by transposing matrix 2->3 (reversing input->output direction) and multiplying it by error\n",
    "        specifically, d is represented by the values in row 1 of matrix 2->3 , [1,2,3]. so the value we want is f*1, g*2, h*3. \n",
    "        for e we want f*4,g*5,h*6. f,g,and h are substituted for e1,e2,and e3, so the operation that gets us what we want is \n",
    "\n",
    "                                                                         [1,4  \n",
    "        D,E = [e1*1+e2*2+e3*3 , e1*4+e2*5+e3*6] = [e1,e2,e3] @            2,5     =  error * transverse(matrix2->3)\n",
    "                                                                          3,6]\n",
    "\n",
    "        neurons[i-1] =  { neurons[i] @ transpose(^that) }  @ derivative(neurons[i-1])\n",
    "\n",
    "        for subsequent neuron layers\n",
    "            values layer backprop from : [d,e]\n",
    "            values layer backprop to : [a,b,c]\n",
    "\n",
    "\n",
    "        adjustments = delta * transpose(matrix 1->2) = [D, E] @ [1,3,5  = [D*1 + E*2, D*3 + E*4, D*5 + E*6] = \n",
    "                                                                 2,4,6]\n",
    "        matrix 1->2 += adjustments * -1\n",
    "        \n",
    "'''                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a numpy array of values and returns a numpy array of the same length. \n",
    "def Softmax(inputs):\n",
    "    return np.exp(inputs)/np.sum(inputs)\n",
    "\n",
    "def RelU(input):\n",
    "    return np.max(input,0)\n",
    "\n",
    "def LeakyRelU(input): #maybe dont use this? More of a demonstration of what should be done inline\n",
    "    #return input * .01 if input < 0 else input\n",
    "    return 1.0/(1+np.exp(-input)) #testing sigmoid out\n",
    "    #return  np.nan_to_num(    np.clip(  ((input < 0) ) * .99 * input + .01 * input , a_min=-10, a_max=10  ) )\n",
    "    \n",
    "    \n",
    "\n",
    "def LeakyRelUDeriv(input):\n",
    "    #return .01 if input < 0 else 1\n",
    "    #return 1 - .99 * (input < 0) #true one\n",
    "    return input * (1.0 - input) #sigmoid test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    #layers is a list of ints. each int represents the number of neurons in that layer.\n",
    "    #the first layer must be the number of columns in the input data\n",
    "    #the last represents the number of possible outputs\n",
    "    def __init__(self, layers, learningrate = .01):\n",
    "        self.learningrate = learningrate\n",
    "        self.vals = list()                                                  #the '\"neurons\". \n",
    "        self.layers = list()                                                #the \"axons and dentrites\".\n",
    "        self.numlayers = len(layers)                                        #paradoxically , len(self.layers) = self.numlayers - 1\n",
    "        self.layersizes = layers\n",
    "        for i in range(len(layers)):\n",
    "            self.vals.append(np.zeros((1,layers[i] +1)))                      #our list of row vectors to serve as inputs to the next layer. +1 to add bias value \n",
    "            if i > 0 :\n",
    "                self.layers.append(np.random.rand(layers[i-1] + 1,layers[i] +1 ) * 1/ ((layers[i] ) * (layers[i-1])))   #our matrix to propagate forward from layer n-1 to layer n.\n",
    "        return\n",
    "\n",
    "    def predict(self,input):\n",
    "        #forward propagation\n",
    "                                                                           #inputs shape should be of shape [1,n]\n",
    "        np.copyto(self.vals[0][:,0:self.layersizes[0]], input)           #copy values into our vals[0], leaving the last spot for bias\n",
    "        self.vals[0][:,-1] = 1                                                  #store our 1 for bias\n",
    "        for i in range(1,self.numlayers):\n",
    "            self.vals[i] = self.vals[i-1] @ self.layers[i-1]\n",
    "            self.vals[i] = LeakyRelU(self.vals[i])  if i < self.numlayers -1 else Softmax(self.vals[i])\n",
    "            self.vals[i][:,-1] = 1\n",
    "        return np.copy(self.vals[-1][:, : self.layersizes[-1]])                #just return a copy of the last layer without the bias value\n",
    "\n",
    "    #each row of input should be a row that can be input into predict - each row of testvals a single value that is the correct one. \n",
    "    #specifically, testvals indicates an index in the output array that should be 1, while the rest are 0. \n",
    "    def train(self, input, testvals, epochs = 1):\n",
    "        #get predictions on each row of input\n",
    "        predictions = np.zeros(testvals.shape[0])\n",
    "        for i in range(epochs):\n",
    "            totalerror = 0\n",
    "            for j in range(input.shape[0]):\n",
    "                if(j%1000==0):\n",
    "                    print(\"Progress : %d\"%(float(j)/input.shape[0]))\n",
    "                p = self.predict(input[j])\n",
    "                #compare them to testvals to get error\n",
    "                #print(\"Predictions shape is \" , p.shape)\n",
    "                err = np.zeros( (1, self.layersizes[-1]))\n",
    "                #print(\"Error shape is \" , err.shape)\n",
    "                err[0,testvals[j]] = 1\n",
    "                err = err-p\n",
    "                err =np.square(err)*1/len(err)                                   #get mse of errors\n",
    "\n",
    "                #feed error into backpropagate\n",
    "                self.backpropagate(err)\n",
    "                totalerror+= np.sum(err)/input.shape[0]\n",
    "            #print mean error and \n",
    "            print(totalerror)\n",
    "        return\n",
    "\n",
    "    def test(self, input, testvals):\n",
    "        return\n",
    "\n",
    "    def backpropagate(self, error):\n",
    "        #print(\"Error is \" , error)\n",
    "        #for 2nd to last layer\n",
    "        #for each previous one\n",
    "        #print(\"Cheaply debugging, its \" , (self.learningrate * np.transpose(self.vals[-2][:,:self.vals[-2].shape[1]-1]) @ error  ).shape)\n",
    "        #print(\"And layers is \", self.layers[-1].shape)\n",
    "        #update transition layer to final x using errors\n",
    "        self.layers[-1][:self.layersizes[-2] , :self.layersizes[-1] ] -= self.learningrate * np.transpose(self.vals[-2][:,:self.vals[-2].shape[1]-1]) @ error     \n",
    "        for i in range( self.numlayers-2,0, -1 ):\n",
    "            derivative = LeakyRelUDeriv(self.vals[i])\n",
    "            d = error @ np.transpose(self.layers[i][:,:self.layersizes[i+1]])\n",
    "            d = d[:d.shape[1],]                                                                                   #slice off bias value\n",
    "            delta = d * derivative                                                                                #element wise product. \n",
    "            #print(\"Delta is \" , delta.shape)\n",
    "            #print(\"Layer i-1 shape is \" , self.layers[i-1].shape)\n",
    "            #print(\"Layer i shape is \" , self.layers[i].shape)\n",
    "            print(delta)\n",
    "            self.layers[i-1] += self.learningrate * -1 *np.transpose(self.vals[i-1]) @delta  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIP. using this vid as resource : https://www.youtube.com/watch?v=9RN2Wr8xvro&list=PL-nR3Zo5zPQvaNGqElO9-N-1z-4N94qBi&index=1\n",
    "#but trying to make it easier to use, more general, commented, and without retarded variable naming conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnimg = mnist.train_images().reshape(60000,28**2)\n",
    "mnlabel = mnist.train_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Network([28**2,200,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[326.6435912  326.73873643 326.61554108 326.62863053 326.76071857\n  326.73489849 326.65712601 326.73379726 326.62713133 326.69312815]]\n"
     ]
    }
   ],
   "source": [
    "print(network.predict(np.reshape(mnimg[0], (1,28**2) )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.09995724 0.09998635 0.09994865 0.09995266 0.09999308 0.09998518\n  0.09996138 0.09998484 0.0999522  0.0999724  0.00030601]]\n"
     ]
    }
   ],
   "source": [
    "print(network.vals[2]/np.sum(network.vals[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#network.train(mnimg[:10],mnlabel[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "layersizes length 2\nneurons length 2\naxons length 1\nbiases length 2\n"
     ]
    }
   ],
   "source": [
    "class NetworkV2:\n",
    "    def __init__(self, layersizes, learningrate = .01):\n",
    "        self.neurons = list()           #list of row vectors\n",
    "        self.axons = list()             #matrix connect neurons[i] to neurons[i+1]. dimension is layersizes[i] , layersizes[i+1]\n",
    "        self.layersizes = layersizes    #the count of neurons in each layer\n",
    "        self.biases = list()            #bonus connection to each neuron in each layer. same shape as self.neurons\n",
    "        self.learningrate = .01\n",
    "        for i in range(len(layersizes)):\n",
    "            self.neurons.append(np.zeros( (1,layersizes[i]) ))\n",
    "            self.biases.append( np.random.rand(1,layersizes[i])/ (self.layersizes[i]) )\n",
    "            if(i>0):\n",
    "                self.axons.append(np.ones( (self.layersizes[i-1],self.layersizes[i]) ) / (self.layersizes[i-1]) )\n",
    "        #debug ifo\n",
    "        print(\"layersizes length %d\" %(len(layersizes)))\n",
    "        print(\"neurons length %d\" %(len(self.neurons)) )\n",
    "        print(\"axons length %d\" %(len(self.axons)) )\n",
    "        print(\"biases length %d\" %(len(self.biases)) )\n",
    "\n",
    "    def predict(self, input):\n",
    "        '''\n",
    "        def predict(self, input):\n",
    "        input should be a numpy array of shape 1,inputsize. remember to normalize input to 0-1 range\n",
    "        '''\n",
    "        np.copyto(self.neurons[0], input)\n",
    "        for i in range(1,len(self.layersizes)):\n",
    "            self.neurons[i] = LeakyRelU(self.neurons[i-1] @ self.axons[i-1]) + self.biases[i]       #nk km nm\n",
    "            \n",
    "            self.neurons[i] = np.nan_to_num(self.neurons[i],posinf=10,neginf=-10)                       #clear nan\n",
    "        return np.copy(self.neurons[-1])\n",
    "\n",
    "    def backpropagate(self,errors):\n",
    "        '''def backpropagate(self,errors):'''\n",
    "        np.copyto(self.neurons[-1], errors)\n",
    "        for i in range(len(self.layersizes)-1,0,-1 ):\n",
    "            self.axons[i-1] += np.transpose(-self.learningrate * self.neurons[i-1]) @ self.neurons[i]               #nk km nm  so Trans(neurons[i-1] ) @ neurons[i] \n",
    "            self.biases[i] += -self.learningrate * self.neurons[i]                                                  #update biases\n",
    "            deriv  = (LeakyRelUDeriv(self.neurons[i-1]) if i < len(self.layersizes)-1 else self.neurons[i-1])       #get derivative of this layer of neurons\n",
    "            self.neurons[i-1] = deriv * (self.neurons[i] @ np.transpose(self.axons[i-1]))                           #backpropagate errors into previous layer\n",
    "            \n",
    "            self.axons[i-1] = np.nan_to_num(self.axons[i-1], posinf=10, neginf=-10)                                 #get rid of weird values.\n",
    "            self.biases[i-1] = np.nan_to_num(self.biases[i-1], posinf=10, neginf=-10)\n",
    "            self.neurons[i-1] = np.nan_to_num(self.neurons[i-1], posinf=10, neginf=-10)\n",
    "        return\n",
    "    \n",
    "    def train(self,vals,labels,epochs = 1):\n",
    "        '''\n",
    "        def train(self,vals,labels):\n",
    "        gradient descent styled training. labels should be indexes of the output that are \"correct\" \n",
    "        '''\n",
    "        originallearningrate = self.learningrate\n",
    "        for e in range(epochs):\n",
    "            #self.learningrate/=1.5\n",
    "            for i in range(vals.shape[0]):\n",
    "                #print(i)\n",
    "                p = Softmax(self.predict(vals[i]))\n",
    "                correct = np.zeros((1,self.layersizes[-1]))\n",
    "                correct[0,labels[i]] = 1\n",
    "                correct = Softmax(correct)\n",
    "                err = p - correct\n",
    "                err = np.nan_to_num(err, posinf=10, neginf=-10)\n",
    "                self.backpropagate(err)\n",
    "        self.learningrate = originallearningrate\n",
    "network2 = NetworkV2([28**2,10],learningrate =.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "mnimg[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "layersizes length 2\nneurons length 2\naxons length 1\nbiases length 2\n"
     ]
    }
   ],
   "source": [
    "network2 = NetworkV2([28**2,10], learningrate=.01)\n",
    "network2.train(mnimg[0:10000,:]/256 , mnlabel[0:10000], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.00127551 0.00127551 0.00127551 ... 0.00127551 0.00127551 0.00127551]\n [0.00127551 0.00127551 0.00127551 ... 0.00127551 0.00127551 0.00127551]\n [0.00127551 0.00127551 0.00127551 ... 0.00127551 0.00127551 0.00127551]\n ...\n [0.00127551 0.00127551 0.00127551 ... 0.00127551 0.00127551 0.00127551]\n [0.00127551 0.00127551 0.00127551 ... 0.00127551 0.00127551 0.00127551]\n [0.00127551 0.00127551 0.00127551 ... 0.00127551 0.00127551 0.00127551]]\n"
     ]
    }
   ],
   "source": [
    "print(network2.axons[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.223413  , 0.25552028, 0.33001456, 0.33164668, 0.2292337 ,\n",
       "        0.25115304, 0.217736  , 0.24619943, 0.29508435, 0.42519487]])"
      ]
     },
     "metadata": {},
     "execution_count": 365
    }
   ],
   "source": [
    "Softmax(network2.predict(mnimg[5:6,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 363
    }
   ],
   "source": [
    "mnlabel[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 207
    }
   ],
   "source": [
    "np.argmax([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "588\n1000\n"
     ]
    }
   ],
   "source": [
    "numcorrect= 0\n",
    "total = 0\n",
    "for i in range(21000,22000,1):\n",
    "    if np.argmax(network2.predict(mnimg[i:i+1,:]))==mnlabel[i]:\n",
    "        numcorrect+=1\n",
    "    total+=1\n",
    "print(numcorrect)\n",
    "print(total)\n",
    "\n",
    "#using softmax in the training funciton helped somewhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  network2.axons[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}