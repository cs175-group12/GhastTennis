{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37432bitedb058b449034df283b6fb9d9a5aa7cb",
   "display_name": "Python 3.7.4 32-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random([1000]) #coming up with arbitrary data and a predictor \n",
    "data = np.reshape(data, (100,10)) #reshape to be 100 rows of 10 cols\n",
    "predict = np.zeros(100)\n",
    "for i in range(len(data)):\n",
    "    predict[i] = float(np.random.random()-.5)\n",
    "    data[i] /= np.sum(data[i]) + predict[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0015994014628424\n1.0038626387317213\n1.025040329790093\n0.9860316001651155\n1.06140662711754\n0.905214551373955\n"
     ]
    }
   ],
   "source": [
    "class Network:\n",
    "    #layers is a list of ints. each int represents the number of neurons in that layer.\n",
    "    #each neuron is linked to every neuron in the previous and next. \n",
    "    #the first layer must be the number of columns in the input data\n",
    "    #the last represents the number of possible outputs\n",
    "    def __init__(self, layers, learningrate = .01):\n",
    "        #layers[0] to layers[1] . lets say 4 to 3\n",
    "        #assume i have a vector of 4 inputs. [-1,1,-1,-2]  also 1 bias [1]\n",
    "        #this needs to get multiplied by a column vector of [.2,.2,. 2,.2,.2] Transpose. each column represents a neuron in the next                    layer. \n",
    "        #so our matrix between layer 0 and layer 1 needs [layers[1]] columns and layers[0] rows. \n",
    "        #we'll get slightly better performance if we do the transverse of that and take input as a column so NEURONS * INPUT = OUTPUT\n",
    "        #so input is a column vector instead and each inbetween-layer is a matrix of layers[n]rows and layers[n-1] columns \n",
    "        self.learningrate = learningrate\n",
    "        self.vals = list()\n",
    "        self.layers = list()\n",
    "        for i in range(len(layers)):\n",
    "            self.vals.append(np.zeros((layers[i],1)) #our list of column vectors to serve as inputs to the next layer. \n",
    "            if(i>0):\n",
    "                self.layers.append(np.ones((layers[n],layers[n-1] + 1))) #our matrix to propagate forward from layer n-1 to layer n.\n",
    "                #the +1 is for a value to store a bias\n",
    "        return\n",
    "    def predict(input):\n",
    "        return\n",
    "\n",
    "    def train(input, testvals):\n",
    "        return\n",
    "\n",
    "    def test(input, testvals):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIP. using this vid as resource : https://www.youtube.com/watch?v=9RN2Wr8xvro&list=PL-nR3Zo5zPQvaNGqElO9-N-1z-4N94qBi&index=1\n",
    "#but trying to make it easier to use, more general, commented, and without retarded variable naming conventions"
   ]
  }
 ]
}