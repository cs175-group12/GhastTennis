{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "16eb897c26cdfcf18817bc60a8e0737e3939ff1e8491198c807979170104e811"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random([1000]) #coming up with arbitrary data and a predictor \n",
    "data = np.reshape(data, (100,10)) #reshape to be 100 rows of 10 cols\n",
    "predict = np.zeros(100)\n",
    "for i in range(len(data)):\n",
    "    predict[i] = float(np.random.random()-.5)\n",
    "    data[i] /= np.sum(data[i]) + predict[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drawing : \n",
    "'''     network : dense                    matrices                                  rows correspond to input layer size + bias, columns to output layer size\n",
    "                                                                                     X is a bias, it should be a constant 1. pad it onto the back of output.\n",
    "        1       2       3                    1->2           2->3\n",
    "        [a]             [f]                 [1 , 2         [1, 2, 3 \n",
    "        [b]     [d]     [g]                  3 , 4          4, 5, 6\n",
    "        [c]     [e]     [h]                  5 , 6          7, 8, 9]\n",
    "        [X]     [X]     [X]                  7 , 8]\n",
    "        \n",
    "        forward propagation                                                         each row of input1 corresponds to a row of output. \n",
    "                            [1, 2        [1a + 3b + 5c + 7X ,2a + 4b + 6c + 8X]     each col of input2 corresponds to a col of output\n",
    "                             3, 4   =                                               [1,input layer + bias] @ [input layer + bias, outputlayer] = [1,ouputlayer]\n",
    "        [a, b, c, X]    x    5, 6      \n",
    "                             7, 8]                                      intuitively, each neuron is represented by a column of the connecting matrix\n",
    "                                                                        with each row within representing an axon from each row in input. \n",
    "        \n",
    "        first make sure vals[i][-1] = 1\n",
    "        intuitively, vals[i] @ layers[i] = vals[i+1]\n",
    "        then pad output layer with a 1.\n",
    "\n",
    "        back propagation\n",
    "            Error : [e1, e2, e3] = [?-f , ?-g , ?-h]\n",
    "            Values : [[input1, input2, input3, X] , [d, e, X] , [f, g, h, X]]\n",
    "            -1 * learning_rate * [derivative_of_output] @ [error] @ transpose([value]) = adjustment\n",
    "            for layer n, output is values[n+1], input is values[n]\n",
    "\n",
    "                                   [e1                           [e1 * f, e1 * g, e1 * h\n",
    "        transpose(error) @ value =  e2    x    [f, g, h]    =     e2 * f, e2 * g, e2 * h\n",
    "                                    e3]                           e3 * f, e3 * g, e3 * h]\n",
    "\n",
    "        matrix 2->3 += ^that * learningrate * -1\n",
    "\n",
    "        for subsequent layers\n",
    "            values layer backprop from : [d,e]\n",
    "            values layer backprop to : [a,b,c,x]\n",
    "\n",
    "        derivative = derivative([d,e])\n",
    "\n",
    "        calculate new d,e from errors by transposing matrix 2->3 (reversing input->output direction) and multiplying it by error\n",
    "        specifically, d is represented by the values in row 1 of matrix 2->3 , [1,2,3]. so the value we want is f*1, g*2, h*3. \n",
    "        for e we want f*4,g*5,h*6. f,g,and h are substituted for e1,e2,and e3, so the operation that gets us what we want is \n",
    "\n",
    "                                                                         [1,4,7  \n",
    "        D,E,X = [e1*1+e2*2+e3*3 , e1*4+e2*5+e3*6, extra] = [e1,e2,e3] @   2,5,8     =  error * transverse(matrix2->3)\n",
    "                                                                          3,6,9]\n",
    "\n",
    "        D,E * derivative = delta\n",
    "\n",
    "        adjustments = delta * transpose(matrix 1->2) = [D, E] @ [1,3,5,7  = [D*1 + E*2, D*3 + E*4, D*5 + E*6, D*7 + E*8] = \n",
    "                                                                 2,4,6,8]\n",
    "        matrix 1->2 += adjustments * -1\n",
    "        \n",
    "'''                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    #layers is a list of ints. each int represents the number of neurons in that layer.\n",
    "    #the first layer must be the number of columns in the input data\n",
    "    #the last represents the number of possible outputs\n",
    "    def __init__(self, layers, learningrate = .01):\n",
    "        self.learningrate = learningrate\n",
    "        self.vals = list()                                                  #the '\"neurons\". \n",
    "        self.layers = list()                                                #the \"axons and dentrites\".\n",
    "        self.numlayers = len(layers)                                        #paradoxically , len(self.layers) = self.numlayers - 1\n",
    "        self.layersizes = layers\n",
    "        for i in range(len(layers)):\n",
    "            self.vals.append(np.ones((1,layers[i] +1)))                      #our list of row vectors to serve as inputs to the next layer. +1 to add bias value \n",
    "            if i > 0 :\n",
    "                self.layers.append(np.ones((layers[n-1] + 1,layers[n] )))   #our matrix to propagate forward from layer n-1 to layer n.\n",
    "        return\n",
    "\n",
    "    def predict(self,input):\n",
    "        #forward propagation\n",
    "                                            #inputs shape should be of shape [1,n]\n",
    "        np.copyto(vals[0], input)           #copy values into our vals[0]\n",
    "        vals[-1] = 1                        #store our 1 for bias\n",
    "        for i in range(1,self.numlayers):\n",
    "            vals[i] = vals[i-1] @ layers[i-1]\n",
    "            vals[i] = LeakyRelU(vals[i])  if i < self.numlayers -1 else Softmax(vals[i])\n",
    "            vals[i][:,-1] = 1\n",
    "        return np.copy(vals[len(vals)-1][:, : vals.shape[1]])                #just return a copy of the last layer without the bias value\n",
    "\n",
    "    #each row of input should be a row that can be input into predict - each row of testvals a single value that is the correct one. \n",
    "    def train(self, input, testvals, epochs = 1):\n",
    "        #get predictions on each row of input\n",
    "        predictions = np.zeros(testvals.shape[0])\n",
    "        for i in range(epochs):\n",
    "            totalerror = 0\n",
    "            for row in input:\n",
    "                p = predict(row)\n",
    "                #compare them to testvals to get error\n",
    "                err = testvals-p\n",
    "                err = np.sum(np.square(err))*1/len(err)                                   #get mse of errors\n",
    "\n",
    "                #feed error into backpropagate\n",
    "                backpropagate(err)\n",
    "                totalerror+= np.sum(err)/input.shape[0]\n",
    "            #print mean error and \n",
    "            print(totalerror)\n",
    "        return\n",
    "\n",
    "    def test(self, input, testvals):\n",
    "        return\n",
    "\n",
    "def backpropagate(self, error):\n",
    "    #for 2nd to last layer\n",
    "    #for each previous one\n",
    "    layers[-1] -= self.learningrate * np.transpose(error) @  vals[-1][:vals[-1].shape[1]-1]                   #cut off bias value, update final layer\n",
    "    for i in range( self.numlayers-2,0, -1 ):\n",
    "        derivative = LeakyRelUDeriv(vals[i])\n",
    "        d = error @ np.transpose(self.layers[i])\n",
    "        d = d[:d.shape[1],]                                                                                   #slice off bias value\n",
    "        delta = d * derivative                                                                                #element wise product. \n",
    "        layers[i-1] += learningrate * -1 * np.transpose(layers[i-1])\n",
    "\n",
    "#takes a numpy array of values and returns a numpy array of the same length. \n",
    "def Softmax(inputs):\n",
    "    return np.exp(inputs)/np.sum(inputs)\n",
    "\n",
    "def RelU(input):\n",
    "    return np.max(input,0)\n",
    "\n",
    "def LeakyRelU(input): #maybe dont use this? More of a demonstration of what should be done inline\n",
    "    return input * .01 if input < 0 else input\n",
    "\n",
    "def LeakyRelUDeriv(input):\n",
    "    return .01 if input < 0 else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIP. using this vid as resource : https://www.youtube.com/watch?v=9RN2Wr8xvro&list=PL-nR3Zo5zPQvaNGqElO9-N-1z-4N94qBi&index=1\n",
    "#but trying to make it easier to use, more general, commented, and without retarded variable naming conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}