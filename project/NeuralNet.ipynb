{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "a5cd50f56488077abb2a62091bed4c0728d2d7723f95be5667b7558e1bb1eb1c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random([1000]) #coming up with arbitrary data and a predictor \n",
    "data = np.reshape(data, (100,10)) #reshape to be 100 rows of 10 cols\n",
    "predict = np.zeros(100)\n",
    "for i in range(len(data)):\n",
    "    predict[i] = float(np.random.random()-.5)\n",
    "    data[i] /= np.sum(data[i]) + predict[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0015994014628424\n1.0038626387317213\n1.025040329790093\n0.9860316001651155\n1.06140662711754\n0.905214551373955\n"
     ]
    }
   ],
   "source": [
    "class Network:\n",
    "    #layers is a list of ints. each int represents the number of neurons in that layer.\n",
    "    #each neuron is linked to every neuron in the previous and next. \n",
    "    #the first layer must be the number of columns in the input data\n",
    "    #the last represents the number of possible outputs\n",
    "    def __init__(self, layers, learningrate = .01):\n",
    "        #layers[0] to layers[1] . lets say 4 to 3\n",
    "        #assume i have a vector of 4 inputs. [-1,1,-1,-2]  also 1 bias [1]\n",
    "        #this needs to get multiplied by a column vector of [.2,.2,. 2,.2,.2] Transpose. each column represents a neuron in the next                    layer. \n",
    "        #so our matrix between layer 0 and layer 1 needs [layers[1]] columns and layers[0] rows. \n",
    "        #we'll get slightly better performance if we do the transverse of that and take input as a column so NEURONS * INPUT = OUTPUT\n",
    "        #so input is a column vector instead and each inbetween-layer is a matrix of layers[n]rows and layers[n-1] columns \n",
    "        self.learningrate = learningrate\n",
    "        self.vals = list()\n",
    "        self.layers = list()\n",
    "        self.numlayers = len(layers)\n",
    "        for i in range(len(layers)):\n",
    "            self.vals.append(np.ones((layers[i] + 1,1)) #our list of column vectors to serve as inputs to the next layer. \n",
    "            if(i>0):\n",
    "                self.layers.append(np.ones((layers[n],layers[n-1] + 1))) #our matrix to propagate forward from layer n-1 to layer n.\n",
    "                #the +1 is for a value to store a bias weight. the last value in layers should always be 1.  \n",
    "        return\n",
    "\n",
    "    def predict(self,input):\n",
    "        #forward propagation\n",
    "                                            #inputs shape should be of shape [n,1]\n",
    "        np.copyto(vals[0], input)           #copy values into our vals[0]\n",
    "        vals[len(vals)-1] = 1               #store our 1 for bias\n",
    "        for i in range(1,len(layers)):\n",
    "            vals[i] = np.matmul(layers[i-1], vals[i-1])\n",
    "            vals[i] = LeakyRelU(vals[i])  if i < numlayers else Softmax(vals[i])\n",
    "        return np.copy(vals[len(vals)-1])   #just return a copy of the last layer\n",
    "\n",
    "    #each row of input should be a column that can be input into predict - each row of testvals a single value that is the correct one. \n",
    "    def train(self, input, testvals):\n",
    "        for i in range(len(input)):     #for each row of input data\n",
    "            prediction = predict(np.reshape(i , (len(i),1)))\n",
    "            errors = learningrate * (testvals[i])\n",
    "        predictions = predict(input)\n",
    "        #back propagation now\n",
    "        errors = learningrate  * testvals[]\n",
    "        return\n",
    "\n",
    "    def test(self, input, testvals):\n",
    "        return\n",
    "\n",
    "#takes a numpy array of values and returns a numpy array of the same length. \n",
    "def Softmax(inputs):\n",
    "    return np.exp(inputs)/np.sum(inputs)\n",
    "\n",
    "def RelU(input):\n",
    "    return max(input,0)\n",
    "\n",
    "def LeakyRelU(input) #maybe dont use this? More of a demonstration of what should be done inline\n",
    "    return input * .01 if input < 0 else input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WIP. using this vid as resource : https://www.youtube.com/watch?v=9RN2Wr8xvro&list=PL-nR3Zo5zPQvaNGqElO9-N-1z-4N94qBi&index=1\n",
    "#but trying to make it easier to use, more general, commented, and without retarded variable naming conventions"
   ]
  }
 ]
}